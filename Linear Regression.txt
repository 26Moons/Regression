Linear Regression is a supervised machine-learning technique used to model the relationship between a dependent variable (Y) and one or more independent variables (X).
It assumes that this relationship can be represented using a straight line, making it one of the simplest and most widely used statistical methods.

In simple terms:

Linear regression finds the best-fit straight line through your data that minimizes the total error between actual values and predicted values.

These differences between observed values (Y) and predicted values (Å¶) are called errors or residuals.

ğŸ“Œ Formula of Simple Linear Regression
ğ‘Œ
^
=
ğ‘
0
+
ğ‘
1
ğ‘‹
Y
^
=b
0
	â€‹

+b
1
	â€‹

X

Where:

ğ‘Œ
^
Y
^
 = Predicted value

ğ‘
0
b
0
	â€‹

 = Intercept

ğ‘
1
b
1
	â€‹

 = Slope

ğ‘‹
X = Input variable

Slope formula:

ğ‘
1
=
âˆ‘
(
ğ‘‹
âˆ’
ğ‘‹
Ë‰
)
(
ğ‘Œ
âˆ’
ğ‘Œ
Ë‰
)
âˆ‘
(
ğ‘‹
âˆ’
ğ‘‹
Ë‰
)
2
b
1
	â€‹

=
âˆ‘(Xâˆ’
X
Ë‰
)
2
âˆ‘(Xâˆ’
X
Ë‰
)(Yâˆ’
Y
Ë‰
)
	â€‹


Intercept formula:

ğ‘
0
=
ğ‘Œ
Ë‰
âˆ’
ğ‘
1
ğ‘‹
Ë‰
b
0
	â€‹

=
Y
Ë‰
âˆ’b
1
	â€‹

X
Ë‰
ğŸ“˜ Example

Suppose you are analyzing Study Hours (X) vs Exam Scores (Y).

Hours	Score
2	50
4	65
5	70
7	85
8	88

Linear regression will try to fit a straight line that best predicts the score from the hours studied.

ğŸ“Š Scatter Plot with Errors (Residuals)

Below is the scatter plot showing:

Data points

Regression best-fit line

Vertical error bars showing residuals

(Generated based on synthetic data.)

âœ”ï¸ This visually shows how far each point is from the predicted line.

